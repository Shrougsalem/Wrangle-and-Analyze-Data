{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction\n",
    "Real-world data gathered from a variety of sources and in a variety of formats. In this project we gathered the tweet archive of Twitter user @dog_rates, also known as WeRateDogs with additional tweets information in JSON file and image predictions file. The quality and tidiness issues are assessed, then every issue is cleaned using by breaking it to define, code, test process. This is called data wrangling and this document we will be reporting the data wrangling efforts.\n",
    "\n",
    "Data wrangling report consists of, first, gathering data second, assessing data, and finally cleaning data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Gathering Data\n",
    "In this section the following three pieces of data as described below were gatherd in a Jupyter Notebook titled wrangle_act.ipynb:\n",
    "\n",
    "* **The WeRateDogs Twitter archive.** This file was giving to me and it was downloaded manually from the link: [twitter_archive_enhanced.csv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv)\n",
    "\n",
    "* **The tweet image predictions**, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "* **Additional data from twitter API** \n",
    "I downloaded the tweet's retweet count and favorite (\"like\") count. \n",
    "We're supposed to use the tweet IDs in the WeRateDogs Twitter archive, query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called tweet_json.txt file. Each tweet's JSON data should be written to its own line. Then we read this .txt file line by line into a pandas DataFrame with (at minimum) tweet ID, retweet count, and favorite count. However, i stuggeled with twitter developer account request and downloaded this file manually from udacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Assessing Data\n",
    "In this section, we were inspecting the datasets for two things: data quality issues (i.e. content issues) and lack of tidiness (i.e. structural issues) to get rid of the dirty and messy data.\n",
    "In data quality issues we're checking for the dirty data (e.g. inaccurate, corrupted, duplicate and messing data). According to the quality dimentions we need to consider include, **compeleteness**, **validity**, **accuracy**, and **consistency**. In data tidiness issues we're looking for messy data with structural issues. Where each variable forms column, each observation forms a row and each observational unit forms a table.\n",
    "Some of these issues was result of visual and programatic assessment.\n",
    "    \n",
    "### Assessment Notes \n",
    "#### Quality issues\n",
    "- **Twitter Archive Data**\n",
    "    - Some of the tweets are re-tweeted or a reply tweets, not original rating tweets. (according to the key points, we only need the original rating tweets).\n",
    "        - There are 79 records of `in_reply_to_status_id`,`in_reply_to_user_id ` \n",
    "        - And 188 records of`retweeted_status_id`,`retweeted_status_user_id`, and `retweeted_status_timestamp`.\n",
    "        - The columns `in_reply_to_status_id`,`in_reply_to_user_id ` and `retweeted_status_id`,`retweeted_status_user_id` are in float format.\n",
    "        - `retweeted_status_timestamp` is in object - string format.\n",
    "    - The `timestamp` column data type is object it should be in timestamp format.\n",
    "    - Missing values in `expanded_urls`, 59 tweet without images.\n",
    "    - The `rating_numerator` has values more than denominator 10 (In this rating system it doesn't need to be cleaned (see key points). However, there are some records that have a higher values.\n",
    "    - The `rating_denominator` has values more or less than 10, the accurate rating denominator should be out of 10.\n",
    "    - The `name` column has values that start with lower case letters and probbaly not true names of pet\n",
    "        - There are 109 records that include `a, an, his, the, very, light, officially, actually, by, old, infuriating, all, getting, this, unacceptable, my, just, and, mad, one, not, quite, such, space, life.`\n",
    "    - The `source` column datatype (it should be category).\n",
    "- **Image Predictions Data**\n",
    "    - The number of records 2075. The image prediction records are missing some of the twitter_archive data (I'm assuming it is because of the tweets beyond 1 aug 2017 key points).\n",
    "- **Tweet JSON Data**\n",
    "    - There are 2354 records. The records of tweet data associated to the twitter archive data are missing.\n",
    "\n",
    "#### Tidiness issues\n",
    "- **Twitter Archive Data**\n",
    "    - There're columns for each dog stage `doggo`,`floofer`,`pupper`,`puppo`.\n",
    "    - Replies and retweets are not original tweets we will remove the unnecessary columns related to retweets and reply.\n",
    "    - This should be merge with dog image prediction, and the number of retweets and favourite. \n",
    "- **Image Predictions Data**\n",
    "    - The columns of predictions `p1`,`p2`,`p3` the prediction algorithms and  columns in `p1_conf`, `p2_conf`, `p3_conf` are for prediction confidence. Also, the columns `p1_dog`,`p2_dog`,`p3_dog` if dog or not.\n",
    "    - The image predections will be joined with tweet archive on the `jpg_url`, `breed`, `confidence`, `is_dog`\n",
    "- **Tweet JSON Data**\n",
    "    - The name of the column `id`.\n",
    "    - Need to be merged with the tweets_archive_data set. \n",
    "\n",
    "#### There were further issues that appeared along the process such as, columns that we will not use, missing records after some updates. Most of the assesment was done programatically, however, there are some issues i discovered visually including, the `name` values, the `source` values has html tags, the column `id` in json file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section i started to clean the data (by working on a copy). \n",
    "The programmatic data cleaning process followed.<br/>\n",
    "**Define**: convert our assessments into defined cleaning tasks. These definitions also serve as an instruction list so others (or yourself in the future) can look at your work and reproduce it.<br/>\n",
    "**Code**: convert those definitions to code and run that code.<br/>\n",
    "**Test**: test your dataset, visually or with code, to make sure your cleaning operations worked.<br/>\n",
    "Additional quality and tidiness issues appeard during the process. After there were 5000+ tweets in the dataset we ended with 1954 entries in the clean dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clean data was stored to `twitter_archive_master.csv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
